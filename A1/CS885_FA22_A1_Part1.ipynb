{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "# Implementation of **Value Iteration**, **Policy Iteration** and **Modified Policy Iteration(MDP)** algorithms.\n",
    "\n",
    "The following is the MDP.py implementation from the sceleton code in A1 part 1.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "czjfnvXLuAwI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "3_-noR_joLbX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    '''A simple MDP class.  It includes the following members'''\n",
    "\n",
    "    def __init__(self, T, R, discount):\n",
    "        '''Constructor for the MDP class\n",
    "\n",
    "        Inputs:\n",
    "        T -- Transition function: |A| x |S| x |S'| array\n",
    "        R -- Reward function: |A| x |S| array\n",
    "        discount -- discount factor: scalar in [0,1)\n",
    "\n",
    "        The constructor verifies that the inputs are valid and sets\n",
    "        corresponding variables in a MDP object'''\n",
    "\n",
    "        assert T.ndim == 3, \"Invalid transition function: it should have 3 dimensions\"\n",
    "        self.nActions = T.shape[0]\n",
    "        self.nStates = T.shape[1]\n",
    "        assert T.shape == (\n",
    "        self.nActions, self.nStates, self.nStates), \"Invalid transition function: it has dimensionality \" + repr(\n",
    "            T.shape) + \", but it should be (nActions,nStates,nStates)\"\n",
    "        assert (abs(T.sum(\n",
    "            2) - 1) < 1e-5).all(), \"Invalid transition function: some transition probability does not equal 1\"\n",
    "        self.T = T\n",
    "        assert R.ndim == 2, \"Invalid reward function: it should have 2 dimensions\"\n",
    "        assert R.shape == (self.nActions, self.nStates), \"Invalid reward function: it has dimensionality \" + repr(\n",
    "            R.shape) + \", but it should be (nActions,nStates)\"\n",
    "        self.R = R\n",
    "        assert 0 <= discount < 1, \"Invalid discount factor: it should be in [0,1)\"\n",
    "        self.discount = discount\n",
    "\n",
    "    def valueIteration(self, initialV, nIterations=np.inf, tolerance=0.01):\n",
    "        '''Value iteration procedure\n",
    "        V <-- max_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "        policy = np.zeros(len(initialV), dtype=int)\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"===========================================================================\")\n",
    "        print(\"Executing Value Iteration\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        while iterId < nIterations and epsilon > tolerance:\n",
    "            Ta_V = np.matmul(self.T, V)\n",
    "            gamma_Ta_V = self.discount * Ta_V\n",
    "            all_possible_values = self.R + gamma_Ta_V\n",
    "            policy = np.argmax(all_possible_values, axis=0)  # Choose the best actions for each state, policy means keep\n",
    "            V_new = np.amax((all_possible_values), axis=0)  # Choose the best action values for each state\n",
    "            # np.round/np.around does not work for 0.5 so not reducing to 2 decimal places\n",
    "            V_diff = (V_new - V)\n",
    "            V = V_new\n",
    "            epsilon = np.linalg.norm(V_diff, np.inf)\n",
    "            #print(\"Iteration : \" + str(iterId) + \" V : \" + str(V) + \" Policy: \" + str(policy) + \" epsilon: \" + str(epsilon))\n",
    "            iterId = iterId + 1\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Final State values after \" + str(iterId) + \" iterations , V: \" + str(V) + \" Policy: \" + str(policy))\n",
    "        print(\"===========================================================================\")\n",
    "        \"\"\"\n",
    "\n",
    "        return [V, iterId, epsilon]\n",
    "\n",
    "    def extractPolicy(self, V):\n",
    "        '''Procedure to extract a policy from a value function\n",
    "        pi <-- argmax_a R^a + gamma T^a V\n",
    "\n",
    "        Inputs:\n",
    "        V -- Value function: array of |S| entries\n",
    "\n",
    "        Output:\n",
    "        policy -- Policy: array of |S| entries'''\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"***************************************************************\")\n",
    "        print(\"Executing Policy Extraction\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        all_possible_values = (self.R + (self.discount*(np.matmul(self.T, V))))  # Get values for all possible state transition in this state\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"All Values for All possible state transition: \")\n",
    "        print(all_possible_values)\n",
    "        \"\"\"\n",
    "\n",
    "        policy = np.argmax(all_possible_values, axis=0)  # Choose the best actions for each state, policy means keep\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Extracted Policy : \" + str(policy))\n",
    "        print(\"--------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # track of action chosen at timestamp t, instead of choosing only value\n",
    "        max_values = [all_possible_values[policy[i]][i] for i in range(len(policy))]\n",
    "        print(\"Values Corresponding to Selected Policies: \" + str(max_values))\n",
    "        print(\"***************************************************************\")\n",
    "        \"\"\"\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def evaluatePolicy(self, policy):\n",
    "        '''Evaluate a policy by solving a system of linear equations\n",
    "        V^pi = R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Input:\n",
    "        policy -- Policy: array of |S| entries\n",
    "\n",
    "        Ouput:\n",
    "        V -- Value function: array of |S| entries'''\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"***************************************************************\")\n",
    "        print(\"Executing Policy Evaluation\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Policy : \" + str(policy))\n",
    "        print(\"Evaluating a policy by solving a system of linear equations\")\n",
    "        \"\"\"\n",
    "\n",
    "        R_policy = np.array([self.R[policy[i]][i] for i in range(len(policy))])\n",
    "        T_policy = np.array([self.T[policy[i]][i] for i in range(len(policy))])\n",
    "        gamma_T_policy = self.discount * T_policy\n",
    "        assert gamma_T_policy.shape[0] == gamma_T_policy.shape[1], \"gamma_T_policy matrix should be square\"\n",
    "        V = np.matmul(np.linalg.inv(np.identity(len(policy)) - gamma_T_policy), R_policy)\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"V : \" + str(V))\n",
    "        print(\"***************************************************************\")\n",
    "        \"\"\"\n",
    "\n",
    "        return V\n",
    "\n",
    "    def policyIteration(self, initialPolicy, nIterations=np.inf):\n",
    "        '''Policy iteration procedure: alternate between policy\n",
    "        evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and policy\n",
    "        improvement (pi <-- argmax_a R^a + gamma T^a V^pi).\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        nIterations -- limit on # of iterations: scalar (default: inf)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar'''\n",
    "\n",
    "        policy = initialPolicy  # np.zeros(self.nStates)\n",
    "        V = np.zeros(self.nStates)\n",
    "        iterId = 0\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"===========================================================================\")\n",
    "        print(\"Executing Policy Iteration\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        \"\"\"\n",
    "\n",
    "        while iterId < nIterations:\n",
    "            V = self.evaluatePolicy(policy)\n",
    "            #print(\"Iteration : \" + str(iterId) + \" V : \" + str(V) + \" policy : \" + str(policy))\n",
    "\n",
    "            policy_new = self.extractPolicy(V)\n",
    "            iterId = iterId + 1\n",
    "\n",
    "            if np.array_equal(policy_new, policy):\n",
    "                break\n",
    "\n",
    "            policy = policy_new\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Iteration : \" + str(iterId) + \" V : \" + str(V) + \" policy : \" + str(policy))\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Final policy after \" + str(iterId) + \" iterations , policy: \" + str(policy))\n",
    "        print(\"===========================================================================\")\n",
    "        \"\"\"\n",
    "\n",
    "        return [policy, V, iterId]\n",
    "\n",
    "    def evaluatePolicyPartially(self, policy, initialV, nIterations=np.inf, tolerance=0.01):\n",
    "        '''Partial policy evaluation:\n",
    "        Repeat V^pi <-- R^pi + gamma T^pi V^pi\n",
    "\n",
    "        Inputs:\n",
    "        policy -- Policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations performed: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        V = initialV  # np.zeros(self.nStates)\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"***************************************************************\")\n",
    "        print(\"Executing Partial Policy Evaluation\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Policy : \" + str(policy))\n",
    "        print(\"Evaluating a policy by repeating \" + str(nIterations) + \" times.\")\n",
    "        print(\"Iteration : \" + str(iterId) + \" V : \" + str(V) + \" Policy: \" + str(policy))\n",
    "        \"\"\"\n",
    "\n",
    "        while iterId < nIterations and epsilon > tolerance:\n",
    "            iterId = iterId+1\n",
    "            R_policy = np.array([self.R[policy[i]][i] for i in range(len(policy))])\n",
    "            T_policy = np.array([self.T[policy[i]][i] for i in range(len(policy))])\n",
    "            Vnew = R_policy + (self.discount * np.matmul(T_policy, V))\n",
    "            epsilon = np.linalg.norm((Vnew-V), np.inf)\n",
    "            V = Vnew\n",
    "\n",
    "            #print(\"Iteration : \" + str(iterId) + \" V : \" + str(V) + \" Policy: \" + str(policy) + \" epsilon : \" + str(epsilon))\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"V : \" + str(V))\n",
    "        print(\"***************************************************************\")\n",
    "        \"\"\"\n",
    "\n",
    "        return [V, iterId, epsilon]\n",
    "\n",
    "    def modifiedPolicyIteration(self, initialPolicy, initialV, nEvalIterations=5, nIterations=np.inf, tolerance=0.01):\n",
    "        '''Modified policy iteration procedure: alternate between\n",
    "        partial policy evaluation (repeat a few times V^pi <-- R^pi + gamma T^pi V^pi)\n",
    "        and policy improvement (pi <-- argmax_a R^a + gamma T^a V^pi)\n",
    "\n",
    "        Inputs:\n",
    "        initialPolicy -- Initial policy: array of |S| entries\n",
    "        initialV -- Initial value function: array of |S| entries\n",
    "        nEvalIterations -- limit on # of iterations to be performed in each partial policy evaluation: scalar (default: 5)\n",
    "        nIterations -- limit on # of iterations to be performed in modified policy iteration: scalar (default: inf)\n",
    "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
    "\n",
    "        Outputs: \n",
    "        policy -- Policy: array of |S| entries\n",
    "        V -- Value function: array of |S| entries\n",
    "        iterId -- # of iterations peformed by modified policy iteration: scalar\n",
    "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
    "\n",
    "        # temporary values to ensure that the code compiles until this\n",
    "        # function is coded\n",
    "        policy = initialPolicy\n",
    "        V = initialV\n",
    "        iterId = 0\n",
    "        epsilon = np.inf\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"===========================================================================\")\n",
    "        print(\"Executing Modified Policy Iteration\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Iteration : \" + str(iterId) + \" policy : \" + str(policy))\n",
    "        \"\"\"\n",
    "\n",
    "        while iterId < nIterations and epsilon > tolerance:\n",
    "            iterId = iterId + 1\n",
    "            Vn, _ , _  = self.evaluatePolicyPartially(policy, V, nEvalIterations, tolerance)\n",
    "            #print(\"Vn : \" + str(Vn))\n",
    "            all_possible_values = (self.R + (self.discount * np.matmul(self.T,Vn)))  # Get values for all possible state transition in this state\n",
    "            policy = np.argmax(all_possible_values, axis=0)  # Choose the best actions for each state, policy means keep\n",
    "\n",
    "            Vn_plus_1 = [all_possible_values[policy[i]][i] for i in range(len(policy))]\n",
    "            V_diff = (Vn_plus_1 - Vn)\n",
    "            V = Vn_plus_1\n",
    "            epsilon = np.linalg.norm(V_diff, np.inf)\n",
    "            #print(\"Iteration : \" + str(iterId) + \" policy : \" + str(policy))\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Final policy after \" + str(iterId) + \" iterations , policy: \" + str(policy))\n",
    "        print(\"===========================================================================\")\n",
    "        \"\"\"\n",
    "\n",
    "        return [policy, V, iterId, epsilon]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing value iteration, policy iteration and modified policy iteration for Markov decision processes with provided example\n",
    "---\n",
    "\n",
    "The following is the TestMDP.py file to test out the the simple MDP example from Lecture 1b Slides 17-18.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "tKIDJe1vt9Cy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "''' Construct simple MDP as described in Lecture 1b Slides 17-18'''\n",
    "# Transition function: |A| x |S| x |S'| array\n",
    "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
    "# Reward function: |A| x |S| array\n",
    "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.9        \n",
    "# MDP object\n",
    "mdp = MDP(T,R,discount)"
   ],
   "metadata": {
    "id": "LepSP64otSI5"
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "Testing valueIteration & extractPolicy procedure using simple MDP example from Lecture 1b Slides 17-18.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "VAL5OIKOuWKI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''Test each procedure'''\n",
    "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n",
    "policy = mdp.extractPolicy(V)\n",
    "print(\"V : \" + str(V))\n",
    "print(\"nIterations : \" + str(nIterations))\n",
    "print(\"Policy : \" + str(policy))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DewtqwB4tbJv",
    "outputId": "dbe94f77-8945-4ab1-b240-125c0b31ade7"
   },
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "V : [31.49636306 38.51527513 43.935435   54.1128575 ]\n",
      "nIterations : 58\n",
      "Policy : [0 1 1 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "Testing evaluatePolicy procedure using simple MDP example from Lecture 1b Slides 17-18. We see the results from evaluatePolicy match from the previous valueIteration and extractPolicy procedures\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "nzFV4Q4muu1f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "V = mdp.evaluatePolicy(np.array([1,0,1,0]))\n",
    "print(\"Value corresponding to policy [1,0,1,0]: \" + str(V))\n",
    "\n",
    "V = mdp.evaluatePolicy(np.array([0,1,1,1]))\n",
    "print(\"Value corresponding to policy [0,1,1,1]: \" + str(V))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SIKxYWrttvI",
    "outputId": "f23c4a65-8063-4478-8fa4-398d72a6f703"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value corresponding to policy [1,0,1,0]: [ 0.          0.         18.18181818 10.        ]\n",
      "Value corresponding to policy [0,1,1,1]: [31.58510431 38.60401638 44.02417625 54.20159875]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "Testing policyIteration procedure using simple MDP example from Lecture 1b Slides 17-18. We see that the policy result and value results match the valueIteration procedure.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "8a0elua1u42_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[policy,V,iterId] = mdp.policyIteration(np.array([0,0,0,0]))\n",
    "print(\"V : \" + str(V))\n",
    "print(\"nIterations : \" + str(iterId))\n",
    "print(\"Epsilon : \" + str(policy))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2cBk04ftd6f",
    "outputId": "b06c4f31-fbdd-4452-8488-1aaf770a7e73"
   },
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "V : [31.58510431 38.60401638 44.02417625 54.20159875]\n",
      "nIterations : 2\n",
      "Epsilon : [0 1 1 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "Testing modifiedPolicyIteration(MDP) procedure using simple MDP example from Lecture 1b Slides 17-18. We see that the policy result and value results match the valueIteration procedure.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "pPUCF9DUwNGJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[V,iterId,epsilon] = mdp.evaluatePolicyPartially(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
    "[policy,V,iterId,tolerance] = mdp.modifiedPolicyIteration(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
    "print(\"V : \" + str(V))\n",
    "print(\"nIterations : \" + str(iterId))\n",
    "print(\"Policy : \" + str(policy))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_0vwyMKvBq3",
    "outputId": "05e0be5d-1d21-4fcd-c951-2aa9763997ea"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "V : [31.50605365934906, 38.524965727978426, 43.945125603197766, 54.12254810271034]\n",
      "nIterations : 11\n",
      "Policy : [0 1 1 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Maze Problem** solving using value iteration, policy iteration and modified policy iteration.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "EFlya6HIwd-I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "''' Construct a simple maze MDP\n",
    "\n",
    "  Grid world layout:\n",
    "\n",
    "  ---------------------\n",
    "  |  0 |  1 |  2 |  3 |\n",
    "  ---------------------\n",
    "  |  4 |  5 |  6 |  7 |\n",
    "  ---------------------\n",
    "  |  8 |  9 | 10 | 11 |\n",
    "  ---------------------\n",
    "  | 12 | 13 | 14 | 15 |\n",
    "  ---------------------\n",
    "\n",
    "  Goal state: 14 \n",
    "  Bad state: 9\n",
    "  End state: 16\n",
    "\n",
    "  The end state is an absorbing state that the agent transitions \n",
    "  to after visiting the goal state.\n",
    "\n",
    "  There are 17 states in total (including the end state) \n",
    "  and 4 actions (up, down, left, right).'''\n",
    "\n",
    "# Transition function: |A| x |S| x |S'| array\n",
    "T = np.zeros([4, 17, 17])\n",
    "a = 0.7;  # intended move\n",
    "b = 0.15;  # lateral move\n",
    "\n",
    "# up (a = 0)\n",
    "\n",
    "T[0, 0, 0] = a + b;\n",
    "T[0, 0, 1] = b;\n",
    "\n",
    "T[0, 1, 0] = b;\n",
    "T[0, 1, 1] = a;\n",
    "T[0, 1, 2] = b;\n",
    "\n",
    "T[0, 2, 1] = b;\n",
    "T[0, 2, 2] = a;\n",
    "T[0, 2, 3] = b;\n",
    "\n",
    "T[0, 3, 2] = b;\n",
    "T[0, 3, 3] = a + b;\n",
    "\n",
    "T[0, 4, 4] = b;\n",
    "T[0, 4, 0] = a;\n",
    "T[0, 4, 5] = b;\n",
    "\n",
    "T[0, 5, 4] = b;\n",
    "T[0, 5, 1] = a;\n",
    "T[0, 5, 6] = b;\n",
    "\n",
    "T[0, 6, 5] = b;\n",
    "T[0, 6, 2] = a;\n",
    "T[0, 6, 7] = b;\n",
    "\n",
    "T[0, 7, 6] = b;\n",
    "T[0, 7, 3] = a;\n",
    "T[0, 7, 7] = b;\n",
    "\n",
    "T[0, 8, 8] = b;\n",
    "T[0, 8, 4] = a;\n",
    "T[0, 8, 9] = b;\n",
    "\n",
    "T[0, 9, 8] = b;\n",
    "T[0, 9, 5] = a;\n",
    "T[0, 9, 10] = b;\n",
    "\n",
    "T[0, 10, 9] = b;\n",
    "T[0, 10, 6] = a;\n",
    "T[0, 10, 11] = b;\n",
    "\n",
    "T[0, 11, 10] = b;\n",
    "T[0, 11, 7] = a;\n",
    "T[0, 11, 11] = b;\n",
    "\n",
    "T[0, 12, 12] = b;\n",
    "T[0, 12, 8] = a;\n",
    "T[0, 12, 13] = b;\n",
    "\n",
    "T[0, 13, 12] = b;\n",
    "T[0, 13, 9] = a;\n",
    "T[0, 13, 14] = b;\n",
    "\n",
    "T[0, 14, 16] = 1;\n",
    "\n",
    "T[0, 15, 11] = a;\n",
    "T[0, 15, 14] = b;\n",
    "T[0, 15, 15] = b;\n",
    "\n",
    "T[0, 16, 16] = 1;\n",
    "\n",
    "# down (a = 1)\n",
    "\n",
    "T[1, 0, 0] = b;\n",
    "T[1, 0, 4] = a;\n",
    "T[1, 0, 1] = b;\n",
    "\n",
    "T[1, 1, 0] = b;\n",
    "T[1, 1, 5] = a;\n",
    "T[1, 1, 2] = b;\n",
    "\n",
    "T[1, 2, 1] = b;\n",
    "T[1, 2, 6] = a;\n",
    "T[1, 2, 3] = b;\n",
    "\n",
    "T[1, 3, 2] = b;\n",
    "T[1, 3, 7] = a;\n",
    "T[1, 3, 3] = b;\n",
    "\n",
    "T[1, 4, 4] = b;\n",
    "T[1, 4, 8] = a;\n",
    "T[1, 4, 5] = b;\n",
    "\n",
    "T[1, 5, 4] = b;\n",
    "T[1, 5, 9] = a;\n",
    "T[1, 5, 6] = b;\n",
    "\n",
    "T[1, 6, 5] = b;\n",
    "T[1, 6, 10] = a;\n",
    "T[1, 6, 7] = b;\n",
    "\n",
    "T[1, 7, 6] = b;\n",
    "T[1, 7, 11] = a;\n",
    "T[1, 7, 7] = b;\n",
    "\n",
    "T[1, 8, 8] = b;\n",
    "T[1, 8, 12] = a;\n",
    "T[1, 8, 9] = b;\n",
    "\n",
    "T[1, 9, 8] = b;\n",
    "T[1, 9, 13] = a;\n",
    "T[1, 9, 10] = b;\n",
    "\n",
    "T[1, 10, 9] = b;\n",
    "T[1, 10, 14] = a;\n",
    "T[1, 10, 11] = b;\n",
    "\n",
    "T[1, 11, 10] = b;\n",
    "T[1, 11, 15] = a;\n",
    "T[1, 11, 11] = b;\n",
    "\n",
    "T[1, 12, 12] = a + b;\n",
    "T[1, 12, 13] = b;\n",
    "\n",
    "T[1, 13, 12] = b;\n",
    "T[1, 13, 13] = a;\n",
    "T[1, 13, 14] = b;\n",
    "\n",
    "T[1, 14, 16] = 1;\n",
    "\n",
    "T[1, 15, 14] = b;\n",
    "T[1, 15, 15] = a + b;\n",
    "\n",
    "T[1, 16, 16] = 1;\n",
    "\n",
    "# left (a = 2)\n",
    "\n",
    "T[2, 0, 0] = a + b;\n",
    "T[2, 0, 4] = b;\n",
    "\n",
    "T[2, 1, 1] = b;\n",
    "T[2, 1, 0] = a;\n",
    "T[2, 1, 5] = b;\n",
    "\n",
    "T[2, 2, 2] = b;\n",
    "T[2, 2, 1] = a;\n",
    "T[2, 2, 6] = b;\n",
    "\n",
    "T[2, 3, 3] = b;\n",
    "T[2, 3, 2] = a;\n",
    "T[2, 3, 7] = b;\n",
    "\n",
    "T[2, 4, 0] = b;\n",
    "T[2, 4, 4] = a;\n",
    "T[2, 4, 8] = b;\n",
    "\n",
    "T[2, 5, 1] = b;\n",
    "T[2, 5, 4] = a;\n",
    "T[2, 5, 9] = b;\n",
    "\n",
    "T[2, 6, 2] = b;\n",
    "T[2, 6, 5] = a;\n",
    "T[2, 6, 10] = b;\n",
    "\n",
    "T[2, 7, 3] = b;\n",
    "T[2, 7, 6] = a;\n",
    "T[2, 7, 11] = b;\n",
    "\n",
    "T[2, 8, 4] = b;\n",
    "T[2, 8, 8] = a;\n",
    "T[2, 8, 12] = b;\n",
    "\n",
    "T[2, 9, 5] = b;\n",
    "T[2, 9, 8] = a;\n",
    "T[2, 9, 13] = b;\n",
    "\n",
    "T[2, 10, 6] = b;\n",
    "T[2, 10, 9] = a;\n",
    "T[2, 10, 14] = b;\n",
    "\n",
    "T[2, 11, 7] = b;\n",
    "T[2, 11, 10] = a;\n",
    "T[2, 11, 15] = b;\n",
    "\n",
    "T[2, 12, 8] = b;\n",
    "T[2, 12, 12] = a + b;\n",
    "\n",
    "T[2, 13, 9] = b;\n",
    "T[2, 13, 12] = a;\n",
    "T[2, 13, 13] = b;\n",
    "\n",
    "T[2, 14, 16] = 1;\n",
    "\n",
    "T[2, 15, 11] = b;\n",
    "T[2, 15, 14] = a;\n",
    "T[2, 15, 15] = b;\n",
    "\n",
    "T[2, 16, 16] = 1;\n",
    "\n",
    "# right (a = 3)\n",
    "\n",
    "T[3, 0, 0] = b;\n",
    "T[3, 0, 1] = a;\n",
    "T[3, 0, 4] = b;\n",
    "\n",
    "T[3, 1, 1] = b;\n",
    "T[3, 1, 2] = a;\n",
    "T[3, 1, 5] = b;\n",
    "\n",
    "T[3, 2, 2] = b;\n",
    "T[3, 2, 3] = a;\n",
    "T[3, 2, 6] = b;\n",
    "\n",
    "T[3, 3, 3] = a + b;\n",
    "T[3, 3, 7] = b;\n",
    "\n",
    "T[3, 4, 0] = b;\n",
    "T[3, 4, 5] = a;\n",
    "T[3, 4, 8] = b;\n",
    "\n",
    "T[3, 5, 1] = b;\n",
    "T[3, 5, 6] = a;\n",
    "T[3, 5, 9] = b;\n",
    "\n",
    "T[3, 6, 2] = b;\n",
    "T[3, 6, 7] = a;\n",
    "T[3, 6, 10] = b;\n",
    "\n",
    "T[3, 7, 3] = b;\n",
    "T[3, 7, 7] = a;\n",
    "T[3, 7, 11] = b;\n",
    "\n",
    "T[3, 8, 4] = b;\n",
    "T[3, 8, 9] = a;\n",
    "T[3, 8, 12] = b;\n",
    "\n",
    "T[3, 9, 5] = b;\n",
    "T[3, 9, 10] = a;\n",
    "T[3, 9, 13] = b;\n",
    "\n",
    "T[3, 10, 6] = b;\n",
    "T[3, 10, 11] = a;\n",
    "T[3, 10, 14] = b;\n",
    "\n",
    "T[3, 11, 7] = b;\n",
    "T[3, 11, 11] = a;\n",
    "T[3, 11, 15] = b;\n",
    "\n",
    "T[3, 12, 8] = b;\n",
    "T[3, 12, 13] = a;\n",
    "T[3, 12, 12] = b;\n",
    "\n",
    "T[3, 13, 9] = b;\n",
    "T[3, 13, 14] = a;\n",
    "T[3, 13, 13] = b;\n",
    "\n",
    "T[3, 14, 16] = 1;\n",
    "\n",
    "T[3, 15, 11] = b;\n",
    "T[3, 15, 15] = a + b;\n",
    "\n",
    "T[3, 16, 16] = 1;\n",
    "\n",
    "# Reward function: |A| x |S| array\n",
    "R = -1 * np.ones([4, 17]);\n",
    "\n",
    "# set rewards\n",
    "R[:, 14] = 100;  # goal state\n",
    "R[:, 9] = -70;  # bad state\n",
    "R[:, 16] = 0;  # end state\n",
    "\n",
    "# Discount factor: scalar in [0,1)\n",
    "discount = 0.95\n",
    "\n",
    "# MDP object\n",
    "mdp = MDP(T, R, discount)\n"
   ],
   "metadata": {
    "id": "WlGGDjn_wwue"
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Calculating **policy**, **value function** and **number of iterations** needed by **value iteration** in Maze Problem\n",
    "\n",
    "---\n",
    "#### Testing Parameters\n",
    "\n",
    "*   Tolerance : 0.01. Used following line in *mdp.valueIteration* procedure\n",
    "```\n",
    "#  tolerance=0.01\n",
    "```\n",
    "*   Initial Value Function: Zero for all states. Used following line in *mdp.valueIteration* procedure\n",
    "```\n",
    "#  initialV=np.zeros(mdp.nStates)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Results\n",
    "\n",
    "1.   **Policy** : [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0]\n",
    "2.   **Value Function** : [ 49.66895295  55.27161487  61.57363146  65.87460207  48.00953133 52.30188839  68.13923599  73.25481858  50.22617115  -0.42481753\n",
    "  77.06611566  81.36344171  66.36179067  76.31383816 100.\n",
    "  89.90583635   0.        ]\n",
    "3.   **Number of Iterations** : 26\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "XX0Ugg4W03ZH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[V, nIterations, epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates), tolerance=0.01)\n",
    "policy = mdp.extractPolicy(V)"
   ],
   "metadata": {
    "id": "79ABaOcl0wKw"
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"==================================================================\")\n",
    "print(\"MDPMaze Value Iteration:\")\n",
    "print(\"Policy \" + str(policy))\n",
    "print(\"Value Function: \" + str(V))\n",
    "print(\"Number of Iterations: \" + str(nIterations))\n",
    "print(\"==================================================================\\n\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUSg8pzn47Lg",
    "outputId": "64adfb66-c626-47fd-b2a8-f3bf433ad509"
   },
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================================================\n",
      "MDPMaze Value Iteration:\n",
      "Policy [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0]\n",
      "Value Function: [ 49.66895295  55.27161487  61.57363146  65.87460207  48.00953133\n",
      "  52.30188839  68.13923599  73.25481858  50.22617115  -0.42481753\n",
      "  77.06611566  81.36344171  66.36179067  76.31383816 100.\n",
      "  89.90583635   0.        ]\n",
      "Number of Iterations: 26\n",
      "==================================================================\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Calculating **policy**, **value function** and **number of iterations** needed by **policy iteration** in Maze Problem\n",
    "---\n",
    "#### Testing Parameters\n",
    "\n",
    "*   Initial Policy: Uses action 0 in all states. Used following line in *mdp.policyIteration* procedure\n",
    "```\n",
    "#  initialPolicy = np.zeros(mdp.nStates, dtype=int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Results\n",
    "\n",
    "\n",
    "1.   **Policy** : [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0]\n",
    "2.   **Value Function** : [ 49.69078867  55.28617892  61.58230087  65.87897994  48.03187576  52.32047965  68.1447605   73.25676304  50.23031164  -0.41942079\n",
    "  77.06767431  81.36397885  66.36430029  76.31513999 100.\n",
    "  89.90596733   0.        ]\n",
    "3.   **Number of Iterations** : 7\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "z7pfq-BR1Jvf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[policy, V, nIterations] = mdp.policyIteration(initialPolicy = np.zeros(mdp.nStates, dtype=int))\n"
   ],
   "metadata": {
    "id": "UpPPbsMn0ylH"
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"==================================================================\")\n",
    "print(\"MDPMaze policy Iteration:\")\n",
    "print(\"Policy \" + str(policy))\n",
    "print(\"Value Function: \" + str(V))\n",
    "print(\"Number of Iterations: \" + str(nIterations))\n",
    "print(\"==================================================================\\n\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCp0LAF051tv",
    "outputId": "a35ed6ca-20ab-4d11-e4d3-e3ac01687535"
   },
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================================================\n",
      "MDPMaze policy Iteration:\n",
      "Policy [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0]\n",
      "Value Function: [ 49.69078867  55.28617892  61.58230087  65.87897994  48.03187576\n",
      "  52.32047965  68.1447605   73.25676304  50.23031164  -0.41942079\n",
      "  77.06767431  81.36397885  66.36430029  76.31513999 100.\n",
      "  89.90596733   0.        ]\n",
      "Number of Iterations: 7\n",
      "==================================================================\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Calculating **number of iterations** to converge when varying number of iterations in partial policy evaluation from 1 to 10 in Maze Problem\n",
    "\n",
    "---\n",
    "\n",
    "#### Testing Parameters\n",
    "\n",
    "*   Tolerance : 0.01 . Used following line in *mdp.modifiedPolicyIteration* procedure\n",
    "```\n",
    "#  tolerance=0.01\n",
    "```\n",
    "*   Initial Policy : Uses action 0 in all states . Used following line in *mdp.modifiedPolicyIteration* procedure\n",
    "```\n",
    "#  initialPolicy = np.zeros(mdp.nStates, dtype=int)\n",
    "```\n",
    "*   Initial Value Function : Assigned 0 to in all states . Used following line in *mdp.modifiedPolicyIteration* procedure\n",
    "```\n",
    "#  initialPolicy = initialV = np.zeros(mdp.nStates)\n",
    "```\n",
    "*   Number of iterations in Partial Policy Evaluation : 0 to 10\n",
    "---\n",
    "\n",
    "#### Results\n",
    "\n",
    "| Number of Iterations in Partial Policy Evaluation | Number of Iterations for Convergence |\n",
    "| --- | --- | \n",
    "| 1 | 14 | \n",
    "| 2 | 10 | \n",
    "| 3 | 8 | \n",
    "| 4 | 8 | \n",
    "| 5 | 7 | \n",
    "| 6 | 7 | \n",
    "| 7 | 7 | \n",
    "| 8 | 6 | \n",
    "| 9 | 7 | \n",
    "| 10 | 7 | \n",
    "\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "_mIUcqgl2gaJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"==================================================================\")\n",
    "print(\"MDPMaze Modifier Policy Iteration:\")\n",
    "for nIterPPE in range(1, 11):\n",
    "    [policy, V, nIterations, epsilon] = mdp.modifiedPolicyIteration(initialPolicy = np.zeros(mdp.nStates, dtype=int),\n",
    "                                                                    initialV = np.zeros(mdp.nStates), nEvalIterations=nIterPPE,\n",
    "                                                                    tolerance=0.01)\n",
    "    print(\"# of Iterations in Partial Policy Evaluation: \" + str(nIterPPE) + \" , # of Iterations for Convergence: \" + str(nIterations))\n",
    "print(\"==================================================================\\n\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F3FWuC00z6n",
    "outputId": "2699b1e7-00e8-4081-aaf1-4d660941a80f"
   },
   "execution_count": 79,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================================================\n",
      "MDPMaze Modifier Policy Iteration:\n",
      "# of Iterations in Partial Policy Evaluation: 1 , # of Iterations for Convergence: 14\n",
      "# of Iterations in Partial Policy Evaluation: 2 , # of Iterations for Convergence: 10\n",
      "# of Iterations in Partial Policy Evaluation: 3 , # of Iterations for Convergence: 8\n",
      "# of Iterations in Partial Policy Evaluation: 4 , # of Iterations for Convergence: 8\n",
      "# of Iterations in Partial Policy Evaluation: 5 , # of Iterations for Convergence: 7\n",
      "# of Iterations in Partial Policy Evaluation: 6 , # of Iterations for Convergence: 7\n",
      "# of Iterations in Partial Policy Evaluation: 7 , # of Iterations for Convergence: 7\n",
      "# of Iterations in Partial Policy Evaluation: 8 , # of Iterations for Convergence: 6\n",
      "# of Iterations in Partial Policy Evaluation: 9 , # of Iterations for Convergence: 7\n",
      "# of Iterations in Partial Policy Evaluation: 10 , # of Iterations for Convergence: 7\n",
      "==================================================================\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OqEVUduNCQhr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "##Discussion\n",
    "---\n",
    "| Convergence Iterations in Value Iteration | Convergence Iterations in Policy Iteration | Convergence Iterations in Partial Policy Iteration | \n",
    "| --- | --- | --- | \n",
    "| 26 | 7 | 6 (For 8 iterations in partial policy evaluation) | \n",
    "\n",
    "In the maze problem, there are a total of |S| = 17 states and |A| = 4 actions. For MDP problem, we see that a k=8 value produces the least number of iterations (6) for convergence for given range of k values (1 to 10). **To recap, k denotes the number of iterations in partial policy evaluation step for MDP.**\n",
    "\n",
    "Increases the number of iterations in partial policy evaluation results in slowly decreasing the number of iterations in MDP for convergence.\n",
    "\n",
    "As number of Actions |A| (= 4) < number of states |S| (=17), runtime for each iteration in \n",
    "*   **Value Iteration** reduces to O(|S|<sup>2</sup>|A|)\n",
    "*   **Policy Iteration** reduces to O(|S|<sup>3</sup>) \n",
    "[As |S| > |A|, |S|<sup>3</sup> > |S|<sup>2</sup>|A| and O(|S|<sup>3</sup> + |S|<sup>2</sup>|A|) reduces to O(|S|<sup>3</sup>)]  \n",
    "*   **MDP** reduces to O(k|S|<sup>2</sup>) when k > |A| (number of partial policy evaluation >= 5) and  O(|S|<sup>2</sup>|A|) when k <= |A| (number of partial policy evaluation is between 1 to 4 inclusive)\n",
    "\n",
    "For **k > 4**, each iteration in MDP will be less complex than policy iteration (k|S|<sup>2</sup> < |S|<sup>2</sup>) until k > |S|, or number of iterations in partial policy evaluation crosses 17. However, between 4 < k < 17, each iteration in MDP will still be more complex than value iteration. \n",
    "\n",
    "For **k <= 4**, each iteration in MDP will be less complex than policy iteration [O(k|S|<sup>2</sup>) < O(|S|<sup>2</sup>)] and value iteration [O(k|S|<sup>2</sup>) < O(|S|<sup>2</sup>|A|)]. \n"
   ],
   "metadata": {
    "id": "LH-grOQo-KIx"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "U2VLWjKx_z9Y"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}